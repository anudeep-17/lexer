# lexer

lexing or tokenization is the process of converting a sequence of characters into a sequence of tokens.
###### [please check my master branch src to look at the code.](https://github.com/anudeep-17/lexer/tree/master)
